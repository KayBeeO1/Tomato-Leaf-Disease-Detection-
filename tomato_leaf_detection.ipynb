{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "NW0Q85PlIIuq"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install matplotlib\n",
        "# !pip install scikit-learn\n",
        "# !pip install opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVBgJLhQIlaf",
        "outputId": "d5c9bb79-aaa6-4330-b420-d5108531eb2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bE22qI1WIt57"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5rWYaCMiSZXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba46793d-702e-426b-f383-6a570d652d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing class folder: Tomato___healthy\n",
            "Found 800 files in Tomato___healthy\n"
          ]
        }
      ],
      "source": [
        "# Function to split data into training, validation, and test sets\n",
        "def split_data(source_dir, train_dir, val_dir, test_dir, split_size=0.15):\n",
        "    for class_folder in os.listdir(source_dir):\n",
        "        class_source_dir = os.path.join(source_dir, class_folder)\n",
        "        if not os.path.isdir(class_source_dir):\n",
        "            continue\n",
        "\n",
        "        print(f'Processing class folder: {class_folder}')\n",
        "        train_class_dir = os.path.join(train_dir, class_folder)\n",
        "        val_class_dir = os.path.join(val_dir, class_folder)\n",
        "        test_class_dir = os.path.join(test_dir, class_folder)\n",
        "\n",
        "        os.makedirs(train_class_dir, exist_ok=True)\n",
        "        os.makedirs(val_class_dir, exist_ok=True)\n",
        "        os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "        file_list = [f for f in os.listdir(class_source_dir) if os.path.isfile(os.path.join(class_source_dir, f))]\n",
        "        print(f'Found {len(file_list)} files in {class_folder}')\n",
        "        if len(file_list) == 0:\n",
        "            print(f'No files found in {class_folder}, skipping.')\n",
        "            continue\n",
        "\n",
        "        train_files, test_files = train_test_split(file_list, test_size=split_size, random_state=42)\n",
        "        val_files, test_files = train_test_split(test_files, test_size=0.5, random_state=42)\n",
        "\n",
        "        for file in train_files:\n",
        "            shutil.copy(os.path.join(class_source_dir, file), os.path.join(train_class_dir, file))\n",
        "        for file in val_files:\n",
        "            shutil.copy(os.path.join(class_source_dir, file), os.path.join(val_class_dir, file))\n",
        "        for file in test_files:\n",
        "            shutil.copy(os.path.join(class_source_dir, file), os.path.join(test_class_dir, file))\n",
        "\n",
        "# Define dataset paths\n",
        "base_dir = '/content/drive/My Drive/Dataset/Tomato/'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Splitting the dataset\n",
        "split_data(base_dir, train_dir, val_dir, test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCVz3QPcnP7o"
      },
      "outputs": [],
      "source": [
        "# base_dir = '/content/drive/My Drive/Crop disease detection Dataset/'\n",
        "# train_dir = os.path.join(base_dir, 'train')\n",
        "# val_dir = os.path.join(base_dir, 'val')\n",
        "# test_dir = os.path.join(base_dir, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP_CcIcDcde_"
      },
      "outputs": [],
      "source": [
        "# Image Data Generators with Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Generate batches of data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_dir))\n",
        "print(f\"Number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_wNupdUIvqp"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction Model using VGG16\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN-eq8LiI6rG"
      },
      "outputs": [],
      "source": [
        "epochs = 40\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb-OKbXCE-H5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCWlm7TxbKvo"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('/content/drive/My Drive/Crop_disease_model.h5')\n",
        "print(\"Model saved successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLNW7I4mbj32"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model # Import the load_model function\n",
        "\n",
        "# Load the model and store in a new variable\n",
        "loaded_model = load_model('/content/drive/My Drive/Crop_disease_model.h5')\n",
        "print(\"Model loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUriDMOyI9Qu"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQJCoKEXJASD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def predict_image(model, img_path, class_labels):\n",
        "    # Extract actual class from image path\n",
        "    actual_class = os.path.basename(os.path.dirname(img_path)).replace('_', ' ')\n",
        "\n",
        "    # Load and display the image\n",
        "    img = mpimg.imread(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Preprocess the image for prediction\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(150, 150))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(prediction)\n",
        "\n",
        "    # Display prediction result\n",
        "    predicted_class_label = class_labels[predicted_class_index]\n",
        "    predicted_class_probability = np.max(prediction)\n",
        "\n",
        "    print('Predicted Class:', predicted_class_label)\n",
        "    print('Predicted Class Probability:', round(predicted_class_probability * 100, 2), '%')\n",
        "    print('Actual Class:', actual_class, '\\n')\n",
        "    print('Prediction Array:', prediction)\n",
        "    print('Predicted Class Index:', predicted_class_index)\n",
        "    print('Image Path:', img_path)\n",
        "\n",
        "# Automatically define class labels from directories\n",
        "def get_class_labels(base_dir):\n",
        "    return sorted(os.listdir(base_dir))\n",
        "\n",
        "# Define dataset paths\n",
        "base_dir = '/content/drive/My Drive/Dataset/Tomato/test'\n",
        "\n",
        "# Get class labels\n",
        "class_labels = get_class_labels(base_dir)\n",
        "print(f\"Class Labels: {class_labels}\")\n",
        "\n",
        "# Example usage\n",
        "image_paths = [\n",
        "    '/content/drive/My Drive/Dataset/Tomato/test/Tomato___Bacterial_spot/image (262).JPG',\n",
        "    '/content/drive/My Drive/Dataset/Tomato/test/Tomato___healthy/image (74).JPG',\n",
        "    '/content/drive/My Drive/Dataset/Tomato/test/Tomato___Early_blight/image (324).JPG',\n",
        "    '/content/drive/My Drive/Dataset/Tomato/test/Tomato___Late_blight/image (205).JPG',\n",
        "]\n",
        "\n",
        "for image_path in image_paths:\n",
        "    predict_image(model, image_path, class_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hseBLpgLYJMw"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=2)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xML83F4mZQTf"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and classification report\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n",
        "\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=list(test_generator.class_indices.keys())))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}